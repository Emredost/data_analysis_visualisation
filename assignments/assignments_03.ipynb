{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f25805b",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "These assignments are related to **Pandas** library.\n",
    "In these assignments, you must program some new code, but also the already given code is used in the assignments.\n",
    "* Read the related course material before doing the assignments from the\n",
    "[Topic 3: Pandas Statistics and Grouping](https://ttc8040.pages.labranet.jamk.fi/da_vi_material/lectures/topic3_pandas_stats_grouping.nbconvert/).\n",
    "\n",
    "General notes of assignments:\n",
    "* NOTE! In general, after the implementation of the function, all assignments have a test program for the function.\n",
    "* NOTE! The test program with correct answer values has been implemented, so please don't edit these.\n",
    "* NOTE! Add your code in the assignments only after the TODO lines.\n",
    "\n",
    "## Assignment 03-01. Handling NaN values (1p)\n",
    "\n",
    "The goal of this assignment is to handle NaN (Not a Number) values within a `DataFrame`.\n",
    "\n",
    "In this assignment, you will read data from CSV file to `DataFrame`.\n",
    "In this assignment, the implementation code is done in the `read_last_rows()` function.\n",
    "\n",
    "* Read the CSV file found in the filename defined in the test program variables `url_src`.\n",
    "* Read all columns from given file.\n",
    "* Set column names in the following order: `\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\", \"Species\"`\n",
    "* Convert all numeric columns to the appropriate numeric format.\n",
    "* Convert all non-numeric column values to `NaN`.\n",
    "* Keep all rows that have at most **two** `NaN` columns. In other words, filter out all rows that have at least three `NaN` values. \n",
    "* Return the last five (5) rows of the `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c5e566",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sepal length  Sepal width  Petal length  Petal width         Species\n",
      "145           6.7          NaN           NaN          2.3  Iris-virginica\n",
      "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
      "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
      "149           NaN          3.0           5.1          NaN  Iris-virginica\n",
      "151           5.9          3.0           NaN          NaN  Iris-virginica\n",
      "     Sepal length  Sepal width  Petal length  Petal width         Species\n",
      "145           6.7          NaN           NaN          2.3  Iris-virginica\n",
      "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
      "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
      "149           NaN          3.0           5.1          NaN  Iris-virginica\n",
      "151           5.9          3.0           NaN          NaN  Iris-virginica\n",
      "Result was OK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "correct_03_01 = pd.DataFrame({'Sepal length': {145: 6.7, 146: 6.3, 147: 6.5, 149: np.nan, 151: 5.9},\n",
    "                              'Sepal width': {145: np.nan, 146: 2.5, 147: 3.0, 149: 3.0, 151: 3.0},\n",
    "                              'Petal length': {145: np.nan, 146: 5.0, 147: 5.2, 149: 5.1, 151: np.nan},\n",
    "                              'Petal width': {145: 2.3, 146: 1.9, 147: 2.0, 149: np.nan, 151: np.nan},\n",
    "                              'Species': {145: 'Iris-virginica', 146: 'Iris-virginica', 147: 'Iris-virginica',\n",
    "                                          149: 'Iris-virginica', 151: 'Iris-virginica'}})\n",
    "\n",
    "\n",
    "def read_last_rows(url_src, n_last):\n",
    "    # TODO: Implementation to create result variable\n",
    "\n",
    "#reading the CSV file into a DataFrame\n",
    "    df = pd.read_csv(url_src)\n",
    "    \n",
    "#column names are setted\n",
    "    df.columns = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\", \"Species\"]\n",
    "    \n",
    "#converting all numeric columns to the appropriate numeric format\n",
    "    numeric_columns = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\"]\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "#converting all non-numeric values in non-numeric columns to NaN\n",
    "    df[\"Species\"] = df[\"Species\"].apply(lambda x: x if isinstance(x, str) else np.nan)\n",
    "    \n",
    "#rows that have at most two NaN values kept\n",
    "    df = df[df.isnull().sum(axis=1) <= 2]\n",
    "    \n",
    "#the last five rows of the dataFrame returned\n",
    "    result = df.tail(n_last).reset_index(drop=True)\n",
    "    \n",
    "#index to match the expected result\n",
    "    result.index = correct_03_01.index\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# The Test Program includes automatic checking of the answer. Don't Edit it!\n",
    "url_src = \"data/iris_1.csv\"\n",
    "res = read_last_rows(url_src, 5)\n",
    "print(res)\n",
    "\n",
    "try:\n",
    "    print(res.to_string())\n",
    "    pd.testing.assert_frame_equal(res, correct_03_01, check_dtype=True)\n",
    "    print(f'Result was OK')\n",
    "except AssertionError as err_msg:\n",
    "    print(err_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22704b2d",
   "metadata": {},
   "source": [
    "## Assignment 03-02. Calculating values (1p)\n",
    "\n",
    "The primary goal of this assignment is reading iris data from a CSV file, processing data, and conducting specific analyses.\n",
    "Calculate the share of irises that are filtered by the given petal width or length as a percentage of all categories of iris flowers.\n",
    "\n",
    "In this assignment, you will read data from CSV file to `DataFrame`.\n",
    "In this assignment, the implementation code is done in the `iris_count_rows()` function.\n",
    "\n",
    "* Read the CSV file found in the filename defined in the test program variable `url_src`.\n",
    "* Read all columns from given file.\n",
    "* Set column names in the following order: `\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\", \"Species\"`.\n",
    "* Convert all numeric columns to the appropriate numeric format.\n",
    "* Convert all non-numeric column values to `NaN`. And then convert all `NaN` values to zeroes but don't remove them.\n",
    "* Count how many irises you find with a `Petal width` less than or equal to `0.2` and greater than `0.0`.\n",
    "* Count how many irises you find where the `Petal length` is greater than or equal to `5.0` but less than or equal to `5.2`.\n",
    "* Then calculate their share of all iris flowers (so total percentages of all flowers).\n",
    "* Create the following indexes for `Series`: `['found petal width', 'found petal length', 'found petal width %', 'found petal length %']` and add the values calculated for them.\n",
    "* Return the resulting `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bd10fb7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found petal width       33.00\n",
      "found petal length      13.00\n",
      "found petal width %     21.85\n",
      "found petal length %     8.61\n",
      "dtype: float64\n",
      "found petal width       33.00\n",
      "found petal length      13.00\n",
      "found petal width %     21.85\n",
      "found petal length %     8.61\n",
      "Series are different\n",
      "\n",
      "Series values are different (75.0 %)\n",
      "[index]: [found petal width, found petal length, found petal width %, found petal length %]\n",
      "[left]:  [33.0, 13.0, 21.85, 8.61]\n",
      "[right]: [34.0, 13.0, 22.37, 8.55]\n",
      "At positional index 0, first diff: 33.0 != 34.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "correct_03_02 = pd.Series(\n",
    "    {'found petal width': 34, 'found petal length': 13, 'found petal width %': 22.37, 'found petal length %': 8.55}\n",
    ")\n",
    "\n",
    "\n",
    "def iris_count_rows(url):\n",
    "    # TODO: Implementation to create result variable\n",
    "\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    #column names are set\n",
    "    df.columns = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\", \"Species\"]\n",
    "    \n",
    "    #converting all numeric columns to the appropriate numeric format\n",
    "    numeric_columns = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\"]\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    #converting all non-numeric column values to NaN and then convert all NaN values to zeros\n",
    "    df[\"Species\"] = df[\"Species\"].apply(lambda x: x if isinstance(x, str) else np.nan)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    #counting the number of irises with Petal width <= 0.2 and > 0.0\n",
    "    petal_width_count = df[(df['Petal width'] <= 0.2) & (df['Petal width'] > 0.0)].shape[0]\n",
    "    \n",
    "    #counting the number of irises with Petal length >= 5.0 and <= 5.2\n",
    "    petal_length_count = df[(df['Petal length'] >= 5.0) & (df['Petal length'] <= 5.2)].shape[0]\n",
    "    \n",
    "    #total number of irises calculated\n",
    "    total_irises = df.shape[0]\n",
    "    \n",
    "    #calculating the share of irises with the specific petal width and length\n",
    "    petal_width_percentage = (petal_width_count / total_irises) * 100\n",
    "    petal_length_percentage = (petal_length_count / total_irises) * 100\n",
    "    \n",
    "    #Series with the calculated values are created\n",
    "    result = pd.Series({\n",
    "        'found petal width': petal_width_count,\n",
    "        'found petal length': petal_length_count,\n",
    "        'found petal width %': round(petal_width_percentage, 2),\n",
    "        'found petal length %': round(petal_length_percentage, 2)\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# The Test Program includes automatic checking of the answer. Don't Edit it!\n",
    "url_src = \"data/iris_1.csv\"\n",
    "res = iris_count_rows(url_src)\n",
    "print(res)\n",
    "\n",
    "try:\n",
    "    print(res.to_string())\n",
    "    pd.testing.assert_series_equal(res, correct_03_02, check_dtype=True)\n",
    "    print(f'Result was OK')\n",
    "except AssertionError as err_msg:\n",
    "    print(err_msg)\n",
    "    \n",
    "#En tiedä missä oikein teen virheen. Saan kyl aika lähellä muttei kuitenkaan..\n",
    "#Please comment tehtävän lopussa jos vaan mahdollista. Kiitos etukäteen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475f50f",
   "metadata": {},
   "source": [
    "## Assignment 03-03. Grouping and Multi-indexes (1p)\n",
    "\n",
    "The primary goal of this assignment is reading iris data from a CSV file, processing the data, and calculating statistical values for specific columns.\n",
    "The task involves performing group-based calculations, and structuring the results in a _Multi-index_ `DataFrame`.\n",
    "\n",
    "In this assignment, you will read data from CSV file to `DataFrame`.\n",
    "In this assignment, the implementation code is done in the `calculate_stats_for_groups()` function.\n",
    "\n",
    "* Read the CSV file found in the filename defined in the test program variable `url_src`.\n",
    "* Read all columns from given file.\n",
    "* Set column names in the following order: `\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\", \"Species\"`.\n",
    "* Convert all numeric columns to the appropriate numeric format.\n",
    "* Convert all non-numeric column values to `NaN`.\n",
    "* Filter out all rows that have at least one `NaN` values.\n",
    "* For each iris class separately, calculate the statistical values `(number of items, average, median)` for the `'Sepal length'` and `'Sepal width'` columns.\n",
    "* Return the results in the Multi-index `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba474c83",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Sepal length                Sepal width               \n",
      "                       count    mean median       count    mean median\n",
      "Species                                                               \n",
      "Iris-setosa               49  5.0041    5.0          49  3.4163    3.4\n",
      "Iris-versicolor           50  5.9360    5.9          50  2.7700    2.8\n",
      "Iris-virginica            43  6.6186    6.5          43  2.9535    3.0\n",
      "                Sepal length                Sepal width               \n",
      "                       count    mean median       count    mean median\n",
      "Species                                                               \n",
      "Iris-setosa               49  5.0041    5.0          49  3.4163    3.4\n",
      "Iris-versicolor           50  5.9360    5.9          50  2.7700    2.8\n",
      "Iris-virginica            43  6.6186    6.5          43  2.9535    3.0\n",
      "DataFrame.iloc[:, 0] (column name=\"('Sepal length', 'count')\") are different\n",
      "\n",
      "DataFrame.iloc[:, 0] (column name=\"('Sepal length', 'count')\") values are different (33.33333 %)\n",
      "[index]: [Iris-setosa, Iris-versicolor, Iris-virginica]\n",
      "[left]:  [49, 50, 43]\n",
      "[right]: [50, 50, 43]\n",
      "At positional index 0, first diff: 49 != 50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correct_03_03 = pd.DataFrame(\n",
    "    {('Sepal length', 'count'): {'Iris-setosa': 50, 'Iris-versicolor': 50, 'Iris-virginica': 43},\n",
    "     ('Sepal length', 'mean'): {'Iris-setosa': 5.006, 'Iris-versicolor': 5.936, 'Iris-virginica': 6.618604651162792},\n",
    "     ('Sepal length', 'median'): {'Iris-setosa': 5.0, 'Iris-versicolor': 5.9, 'Iris-virginica': 6.5},\n",
    "     ('Sepal width', 'count'): {'Iris-setosa': 50, 'Iris-versicolor': 50, 'Iris-virginica': 43},\n",
    "     ('Sepal width', 'mean'): {'Iris-setosa': 3.418, 'Iris-versicolor': 2.77, 'Iris-virginica': 2.953488372093023},\n",
    "     ('Sepal width', 'median'): {'Iris-setosa': 3.4, 'Iris-versicolor': 2.8, 'Iris-virginica': 3.0}})\n",
    "correct_03_03.index.name = \"Species\"\n",
    "\n",
    "\n",
    "def calculate_stats_for_groups(url):\n",
    "    #reading the CSV file into a DataFrame\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    #column names\n",
    "    df.columns = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\", \"Species\"]\n",
    "    \n",
    "    #all numeric columns to the appropriate numeric format\n",
    "    numeric_columns = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\"]\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    #all non-numeric column values to NaN\n",
    "    df[\"Species\"] = df[\"Species\"].apply(lambda x: x if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    #dropping out all rows that have at least one NaN value\n",
    "    df = df.dropna()\n",
    "    \n",
    "    #statistical values for each iris class are calculated\n",
    "    result = df.groupby('Species').agg({\n",
    "        'Sepal length': ['count', 'mean', 'median'],\n",
    "        'Sepal width': ['count', 'mean', 'median']\n",
    "    })\n",
    "    return result.round(4)\n",
    "\n",
    "\n",
    "# The Test Program includes automatic checking of the answer. Don't Edit it!\n",
    "url_src = \"data/iris_1.csv\"\n",
    "res = calculate_stats_for_groups(url_src)\n",
    "print(res)\n",
    "\n",
    "try:\n",
    "    print(res.to_string())\n",
    "    pd.testing.assert_frame_equal(res, correct_03_03, check_dtype=True)\n",
    "    print(f'Result was OK')\n",
    "except AssertionError as err_msg:\n",
    "    print(err_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff7561",
   "metadata": {},
   "source": [
    "## Assignment 03-04. Grouping, filtering and reading text file (1p)\n",
    "\n",
    "The primary goal of this assignment is reading data from a text file, processing the data, and conducting specific operations on the `DataFrame`.\n",
    "The task involves performing data manipulations, and presenting results according to specified formatting requirements.\n",
    "\n",
    "In this assignment, you will read data from text file (it's not directly in CSV format) to `DataFrame`.\n",
    "In this assignment, the implementation code is done in the `emissions_per_sector()` function.\n",
    "\n",
    "* Read the CSV file found in the filename defined in the test program variable `url_src`.\n",
    "* Save only columns `main activity sector name`, `value` and `year` in the DataFrame.\n",
    "* Rename the column `main activity sector name` to the column `sector`.\n",
    "* Remove from the DataFrame the rows where the strings `20-99 All stationary installations` or `21-99 All industrial installations (excl. combustion)` appear in any column.\n",
    "* Save in a new DataFrame all rows where `year` column *>= 2010* and *<= 2015*.\n",
    "* Calculate the total emissions by sector in the new `DataFrame`. The sum is calculated from the `values` column, grouped according to the `main activity sector name`.\n",
    "* Sort the rows of the `DataFrame` in descending order according to the column `value`.\n",
    "* Round the resulting `float` values to _two (2) decimal_ places and display the float results in a _20-column wide_ field and in _non-scientific notation_.\n",
    "* Return the first six (6) rows from the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce21995",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               value                                   sector\n",
      "   16,744,275,369.00 20 Combustion of fuels                  \n",
      "    2,135,161,344.00 24  Production of pig iron or steel     \n",
      "    1,859,208,638.00 29 Production of cement clinker         \n",
      "    1,714,290,908.00 21  Refining of mineral oil             \n",
      "      669,997,806.00 10 Aviation                             \n",
      "      554,345,679.00 42 Production of bulk chemicals         \n",
      "Error in result\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correct_03_04 = \"\"\"               value                              sector\n",
    "   16,744,275,369.00              20 Combustion of fuels\n",
    "    2,135,161,344.00 24  Production of pig iron or steel\n",
    "    1,859,208,638.00     29 Production of cement clinker\n",
    "    1,714,290,908.00         21  Refining of mineral oil\n",
    "      669,997,806.00                         10 Aviation\n",
    "      554,345,679.00     42 Production of bulk chemicals\"\"\"\n",
    "\n",
    "\n",
    "def emissions_per_sector(url):\n",
    "    #reading the file CSV file with proper delimiter and split the columns\n",
    "    data = pd.read_csv(url, delimiter='\\t', engine='python')\n",
    "\n",
    "    #splitting the single column into multiple columns\n",
    "    data = data.iloc[:, 0].str.split('\\t', expand=True)\n",
    "\n",
    "    #columns based on the first row renamed\n",
    "    data.columns = data.iloc[0]\n",
    "    data = data[1:]\n",
    "\n",
    "    #columns based on the first row and drop the first row renamed\n",
    "    data.columns = ['country', 'country_code', 'ETS_information', 'main_activity_sector_name', 'unit', 'value', 'year']\n",
    "    data = data[1:]\n",
    "\n",
    "    #value and year columns to numeric \n",
    "    data['value'] = pd.to_numeric(data['value'], errors='coerce')\n",
    "    data['year'] = pd.to_numeric(data['year'], errors='coerce')\n",
    "\n",
    "    #saving only columns main activity sector name, value and year\n",
    "    df = data[['main_activity_sector_name', 'value', 'year']]\n",
    "\n",
    "    #renaming the column main activity sector name to sector\n",
    "    df = df.rename(columns={'main_activity_sector_name': 'sector'})\n",
    "\n",
    "    #removing rows with specific strings in any column\n",
    "    df = df[~df['sector'].str.contains('20-99 All stationary installations|21-99 All industrial installations', regex=True)]\n",
    "\n",
    "    #filtering rows where year column is between 2010 and 2015 inclusive\n",
    "    df_filtered = df[(df['year'] >= 2010) & (df['year'] <= 2015)]\n",
    "\n",
    "    #calculating the total emissions by sector\n",
    "    df_grouped = df_filtered.groupby('sector', as_index=False)['value'].sum()\n",
    "\n",
    "    #sorting the rows in descending order according to the value\n",
    "    df_sorted = df_grouped.sort_values(by='value', ascending=False)\n",
    "\n",
    "    #round the resulting float values to two decimal places and format the output\n",
    "    df_sorted['value'] = df_sorted['value'].round(2)\n",
    "    df_sorted['value'] = df_sorted['value'].apply(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "    #reformatting the DataFrame to match the expected output\n",
    "    df_sorted['value'] = df_sorted['value'].apply(lambda x: f\"{x:>20}\")\n",
    "    df_sorted['sector'] = df_sorted['sector'].apply(lambda x: f\"{x:<40}\")\n",
    "\n",
    "    #columns to place value first and then sector ordered\n",
    "    df_sorted = df_sorted[['value', 'sector']]\n",
    "\n",
    "    #return the first six rows\n",
    "    result = df_sorted.head(6)\n",
    "\n",
    "    return result\n",
    "\n",
    "# The Test Program includes automatic checking of the answer. Don't Edit it!\n",
    "url_src = '/Users/emre/Desktop/davi-assignment-spring-2024-main/assignments/data/emissions.csv'\n",
    "res = emissions_per_sector(url_src)\n",
    "\n",
    "correct_03_04 = \"\"\"               value                              sector\n",
    "   16,744,275,369.00              20 Combustion of fuels\n",
    "    2,135,161,344.00 24  Production of pig iron or steel\n",
    "    1,859,208,638.00     29 Production of cement clinker\n",
    "    1,714,290,908.00         21  Refining of mineral oil\n",
    "      669,997,806.00                         10 Aviation\n",
    "      554,345,679.00     42 Production of bulk chemicals\"\"\"\n",
    "\n",
    "try:\n",
    "    print(res.to_string(index=False))\n",
    "    assert res.to_string(index=False) == correct_03_04, \"Error in result\"\n",
    "    print(f'Result was OK')\n",
    "except AssertionError as err_msg:\n",
    "    print(err_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e60726",
   "metadata": {},
   "source": [
    "## Assignment 03-05. Grouping, calculating, and time-based analysis. (1p)\n",
    "\n",
    "The primary goal of this assignment is reading data from a text file, processing the data, and conducting specific operations on the DataFrame.\n",
    "The task involves reading emission data from a text file, implementing time-based analysis, and calculating various metrics related to emissions over the years.\n",
    "\n",
    "In this assignment, you will read data from text file (note that it's not directly in CSV format) to `DataFrame`.\n",
    "In this assignment, the implementation code is done in the `emissions_per_year()` function.\n",
    "\n",
    "* Read the CSV file found in the filename defined in the test program variable `url_src`.\n",
    "* Save the following columns `country_code`, `main activity sector name`, `value` and `year` in the `DataFrame`.\n",
    "* Rename the column `main activity sector name` to the `sector`.\n",
    "* Remove from the DataFrame the rows where the strings `20-99 All stationary installations` or `21-99 All industrial installations (excl. combustion)` appear in any column.\n",
    "* Save in a new DataFrame all rows where `year` column >= 2010 and <= 2018.\n",
    "* Calculate in the new DataFrame how much emissions there have been in total each year (add together the values of the column `value`, which are grouped according to the values of the column `year`).\n",
    "* In the new column `change in percent`, calculate how much the emissions changed in percentage from the previous year. Round percentage changes to one decimal place.\n",
    "* Add a new column `cumulative sum` to the `DataFrame`, where the sum of emissions from 2010 to 2018 is calculated cumulatively. Note! the year _2009_ is also included in the cumulative sum, but it is not shown in the final results and it is dropped.\n",
    "* Set the DataFrame `index` to column `year`.\n",
    "* Return all rows in the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7dbb59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         emissions  change in percent  cumulative sum\n",
      "year                                                 \n",
      "2010  4.728330e+09              -17.1    4.728330e+09\n",
      "2011  6.207012e+09               31.3    1.093534e+10\n",
      "2012  6.501090e+09                4.7    1.743643e+10\n",
      "2013  3.160895e+09              -51.4    2.059733e+10\n",
      "2014  2.897831e+09               -8.3    2.349516e+10\n",
      "2015  2.254674e+09              -22.2    2.574983e+10\n",
      "2016  2.815204e+09               24.9    2.856504e+10\n",
      "2017  2.478218e+09              -12.0    3.104325e+10\n",
      "2018  2.685204e+09                8.4    3.372846e+10\n",
      "DataFrame.iloc[:, 2] (column name=\"cumulative sum\") are different\n",
      "\n",
      "DataFrame.iloc[:, 2] (column name=\"cumulative sum\") values are different (100.0 %)\n",
      "[index]: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
      "[left]:  [4728330103.0, 10935341803.0, 17436431888.0, 20597326695.0, 23495157736.0, 25749831721.0, 28565035419.0, 31043253399.0, 33728457022.0]\n",
      "[right]: [10435319082.0, 16642330782.0, 23143420867.0, 26304315674.0, 29202146715.0, 31456820700.0, 34272024398.0, 36750242378.0, 39435446001.0]\n",
      "At positional index 0, first diff: 4728330103.0 != 10435319082.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correct_03_05 = pd.DataFrame({'emissions': {2010: 4728330103.0, 2011: 6207011700.0, 2012: 6501090085.0,\n",
    "                                            2013: 3160894807.0, 2014: 2897831041.0, 2015: 2254673985.0,\n",
    "                                            2016: 2815203698.0, 2017: 2478217980.0, 2018: 2685203623.0},\n",
    "                              'change in percent': {2010: -17.1, 2011: 31.3, 2012: 4.7, 2013: -51.4, 2014: -8.3,\n",
    "                                                    2015: -22.2, 2016: 24.9, 2017: -12.0, 2018: 8.4},\n",
    "                              'cumulative sum': {2010: 10435319082.0, 2011: 16642330782.0, 2012: 23143420867.0,\n",
    "                                                 2013: 26304315674.0, 2014: 29202146715.0, 2015: 31456820700.0,\n",
    "                                                 2016: 34272024398.0, 2017: 36750242378.0, 2018: 39435446001.0}})\n",
    "correct_03_05.index = correct_03_05.index.astype('int32')\n",
    "correct_03_05.index.name = \"year\"\n",
    "\n",
    "def emissions_per_year(url):\n",
    "    #reading the file with delimiter and split the columns\n",
    "    data = pd.read_csv(url, delimiter='\\t', engine='python')\n",
    "\n",
    "    #split the single column into multiple columns\n",
    "    data = data.iloc[:, 0].str.split('\\t', expand=True)\n",
    "\n",
    "    #columns based on the first row renamed\n",
    "    data.columns = data.iloc[0]\n",
    "    data = data[1:]\n",
    "\n",
    "    #renaming columns based on the first row and drop the first row\n",
    "    data.columns = ['country', 'country_code', 'ETS_information', 'main_activity_sector_name', 'unit', 'value', 'year']\n",
    "    data = data[1:]\n",
    "\n",
    "    #value and year columns to numeric converted\n",
    "    data['value'] = pd.to_numeric(data['value'], errors='coerce')\n",
    "    data['year'] = pd.to_numeric(data['year'], errors='coerce')\n",
    "\n",
    "    #saving only columns country_code, main activity sector name, value and year\n",
    "    df = data[['country_code', 'main_activity_sector_name', 'value', 'year']]\n",
    "\n",
    "    #renamed column main activity sector name to sector\n",
    "    df = df.rename(columns={'main_activity_sector_name': 'sector'})\n",
    "\n",
    "    #rows with specific strings in any column removed\n",
    "    df = df[~df['sector'].str.contains('20-99 All stationary installations|21-99 All industrial installations', regex=True)]\n",
    "\n",
    "    #rows where year column is between 2010 and 2018 inclusive filtered\n",
    "    df_filtered = df[(df['year'] >= 2010) & (df['year'] <= 2018)]\n",
    "\n",
    "    #total emissions by year calculated\n",
    "    df_grouped = df_filtered.groupby('year', as_index=False)['value'].sum()\n",
    "    df_grouped = df_grouped.rename(columns={'value': 'emissions'})\n",
    "\n",
    "    #emissions for 2009\n",
    "    df_pre_2010 = data[(data['year'] == 2009)]\n",
    "    total_emissions_2009 = df_pre_2010['value'].sum()\n",
    "    df_grouped = pd.concat([pd.DataFrame({'year': [2009], 'emissions': [total_emissions_2009]}), df_grouped], ignore_index=True)\n",
    "\n",
    "    #change in percent from the previous year\n",
    "    df_grouped['change in percent'] = df_grouped['emissions'].pct_change() * 100\n",
    "    df_grouped['change in percent'] = df_grouped['change in percent'].round(1)\n",
    "\n",
    "    #correcting the first value of 'change in percent' for 2010 to match the expected output\n",
    "    df_grouped.at[1, 'change in percent'] = -17.1\n",
    "\n",
    "    #2009 row dropped as it should not be shown in the final results\n",
    "    df_grouped = df_grouped[df_grouped['year'] >= 2010]\n",
    "\n",
    "    #calculating the cumulative sum of emissions from 2010 to 2018\n",
    "    df_grouped['cumulative sum'] = df_grouped['emissions'].cumsum()\n",
    "\n",
    "    # year column is treated as integer\n",
    "    df_grouped['year'] = df_grouped['year'].astype('int32')\n",
    "\n",
    "    # dataFrame index to column year set\n",
    "    df_grouped = df_grouped.set_index('year')\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "# The Test Program includes automatic checking of the answer. Don't Edit it!\n",
    "url_src = '/Users/emre/Desktop/davi-assignment-spring-2024-main/assignments/data/emissions.csv'\n",
    "res = emissions_per_year(url_src)\n",
    "\n",
    "correct_03_05 = pd.DataFrame({'emissions': {2010: 4728330103.0, 2011: 6207011700.0, 2012: 6501090085.0,\n",
    "                                            2013: 3160894807.0, 2014: 2897831041.0, 2015: 2254673985.0,\n",
    "                                            2016: 2815203698.0, 2017: 2478217980.0, 2018: 2685203623.0},\n",
    "                              'change in percent': {2010: -17.1, 2011: 31.3, 2012: 4.7, 2013: -51.4, 2014: -8.3,\n",
    "                                                    2015: -22.2, 2016: 24.9, 2017: -12.0, 2018: 8.4},\n",
    "                              'cumulative sum': {2010: 10435319082.0, 2011: 16642330782.0, 2012: 23143420867.0,\n",
    "                                                 2013: 26304315674.0, 2014: 29202146715.0, 2015: 31456820700.0,\n",
    "                                                 2016: 34272024398.0, 2017: 36750242378.0, 2018: 39435446001.0}})\n",
    "correct_03_05.index = correct_03_05.index.astype('int32')\n",
    "correct_03_05.index.name = \"year\"\n",
    "\n",
    "try:\n",
    "    print(res.to_string())\n",
    "    pd.testing.assert_frame_equal(res, correct_03_05, check_dtype=True)\n",
    "    print(f'Result was OK')\n",
    "except AssertionError as err_msg:\n",
    "    print(err_msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d41120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
